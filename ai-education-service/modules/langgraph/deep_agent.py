"""
Deep Agent ä¸»æ¨¡å—
ä½œä¸ºä¸»ç³»ç»Ÿæ›¿ä»£åŸæœ‰çš„ LangGraph Supervisor æ¶æ„

é…ç½®é¡¹ï¼š
- model: ä¸» LLM æ¨¡å‹
- tools: è‡ªå®šä¹‰å·¥å…·ï¼ˆæ£€ç´¢ã€è®°å¿†ç­‰ï¼‰
- system_prompt: ç³»ç»Ÿæç¤ºè¯
- subagents: å­ä»£ç†åˆ—è¡¨ï¼ˆåç»­é€ä¸ªæ¥å…¥ï¼‰
- checkpointer: çŸ­æœŸè®°å¿†æŒä¹…åŒ–
- store: é•¿æœŸè®°å¿†æŒä¹…åŒ–
- backend: æ–‡ä»¶ç³»ç»Ÿåç«¯
- middleware: ä¸­é—´ä»¶
- interrupt_on: äººæœºåä½œä¸­æ–­é…ç½®
"""

import logging
from typing import Dict, Any, Optional, List, AsyncGenerator

from deepagents import create_deep_agent, SubAgent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.base import BaseCheckpointSaver
from langgraph.store.base import BaseStore

from config import settings
from .tools import memory_read, memory_write

logger = logging.getLogger(__name__)


# ==================== ç³»ç»Ÿæç¤ºè¯ ====================

EDUCATION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI æ•™è‚²è¾…å¯¼åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©å­¦ç”Ÿå­¦ä¹ æ•™æå†…å®¹ã€‚

## ä½ çš„èƒ½åŠ›

1. **æ•™ææ£€ç´¢** - ä½¿ç”¨ retrieve_from_textbook ä»æ•™æä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹
2. **çŸ¥è¯†å›¾è°±** - ä½¿ç”¨ search_knowledge_graph ç†è§£æ¦‚å¿µä¹‹é—´çš„å…³ç³»
3. **è®°å¿†ç®¡ç†** - ä½¿ç”¨ memory_read/memory_write è¯»å†™ç”¨æˆ·è®°å¿†
4. **ä»»åŠ¡è§„åˆ’** - ä½¿ç”¨å†…ç½®çš„ write_todos åˆ†è§£å¤æ‚ä»»åŠ¡
5. **å­ä»£ç†å§”æ‰˜** - å°†ä¸“ä¸šä»»åŠ¡å§”æ‰˜ç»™å­ä»£ç†ï¼ˆåç»­æ¥å…¥ï¼‰

## å·¥ä½œæµç¨‹

1. åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­æ„å›¾ç±»å‹
2. è¯»å–ç”¨æˆ·è®°å¿†ï¼Œäº†è§£ç”¨æˆ·èƒŒæ™¯
3. ä½¿ç”¨å·¥å…·æ£€ç´¢ç›¸å…³ä¿¡æ¯
4. æ ¹æ®éœ€è¦å§”æ‰˜ç»™ä¸“ä¸šå­ä»£ç†
5. æ•´åˆç»“æœï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
6. å­˜å‚¨é‡è¦ä¿¡æ¯åˆ°ç”¨æˆ·è®°å¿†

## æ„å›¾ç±»å‹

- review_summary: å¤ä¹ æ€»ç»“ï¼ˆç”ŸæˆçŸ¥è¯†ç‚¹æ€»ç»“ã€æ€ç»´å¯¼å›¾ï¼‰
- homework_help: ä½œä¸šè¾…å¯¼ï¼ˆè§£é¢˜ã€åˆ†æã€è¯æ˜ï¼‰
- concept_explain: æ¦‚å¿µè§£é‡Šï¼ˆå®šä¹‰ã€åŸç†ã€ä¾‹å­ï¼‰
- learning_plan: å­¦ä¹ è§„åˆ’ï¼ˆåˆ¶å®šè®¡åˆ’ã€å»ºè®®ï¼‰
- question_answer: é—®é¢˜è§£ç­”ï¼ˆå›ç­”å…·ä½“é—®é¢˜ï¼‰
- exercise_practice: ç»ƒä¹ è®­ç»ƒï¼ˆç”Ÿæˆç»ƒä¹ é¢˜ï¼‰

## è¾“å‡ºè¦æ±‚

- ä½¿ç”¨ä¸­æ–‡å›ç­”
- æ¡ç†æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
- å¼•ç”¨æ¥æºæ—¶ä½¿ç”¨ [æ¥æºX] æ ¼å¼
- æ ¹æ®å­¦ç”Ÿæ°´å¹³è°ƒæ•´è¡¨è¾¾æ–¹å¼
- å¯¹äºæ•°å­¦å…¬å¼ä½¿ç”¨ LaTeX æ ¼å¼

## å½“å‰ä¸Šä¸‹æ–‡

ç”¨æˆ·ID: {user_id}
æ•™æID: {book_id}
æ•™æåç§°: {book_name}
å­¦ç§‘: {book_subject}
"""


# ==================== å…¨å±€å®ä¾‹ ====================

_deep_agent = None
_checkpointer: Optional[BaseCheckpointSaver] = None
_store: Optional[BaseStore] = None


def set_deep_agent_checkpointer(checkpointer: Optional[BaseCheckpointSaver]) -> None:
    """è®¾ç½® Checkpointerï¼ˆç”± main.py è°ƒç”¨ï¼‰"""
    global _checkpointer, _deep_agent
    _checkpointer = checkpointer
    _deep_agent = None  # é‡ç½®ï¼Œä¸‹æ¬¡è°ƒç”¨æ—¶é‡æ–°åˆ›å»º
    logger.info(f"Deep Agent Checkpointer å·²è®¾ç½®: {checkpointer is not None}")


def set_deep_agent_store(store: Optional[BaseStore]) -> None:
    """è®¾ç½® Storeï¼ˆç”± main.py è°ƒç”¨ï¼‰"""
    global _store, _deep_agent
    _store = store
    _deep_agent = None  # é‡ç½®ï¼Œä¸‹æ¬¡è°ƒç”¨æ—¶é‡æ–°åˆ›å»º
    logger.info(f"Deep Agent Store å·²è®¾ç½®: {store is not None}")


def _get_model() -> ChatOpenAI:
    """è·å– LLM æ¨¡å‹"""
    if settings.CHAT_PROVIDER == "dashscope":
        return ChatOpenAI(
            model=settings.CHAT_MODEL,
            api_key=settings.DASHSCOPE_API_KEY,
            base_url=settings.DASHSCOPE_BASE_URL,
        )
    else:
        return ChatOpenAI(
            model=settings.OPENROUTER_CHAT_MODEL,
            api_key=settings.OPENROUTER_API_KEY,
            base_url=settings.OPENROUTER_BASE_URL,
        )


def get_deep_agent():
    """è·å– Deep Agent å•ä¾‹"""
    global _deep_agent
    
    if _deep_agent is not None:
        return _deep_agent
    
    logger.info("åˆ›å»º Deep Agent...")
    
    # è·å–æ¨¡å‹
    model = _get_model()
    
    # å®šä¹‰å·¥å…·ï¼ˆä¸»ç³»ç»Ÿåªç”¨è®°å¿†å·¥å…·ï¼Œæ£€ç´¢å·¥å…·ç”±å­æ™ºèƒ½ä½“ä½¿ç”¨ï¼‰
    tools = [
        memory_read,
        memory_write,
    ]

    # å­ä»£ç†åˆ—è¡¨ï¼ˆåç»­é€ä¸ªæ¥å…¥ï¼‰
    subagents: List[SubAgent] = [
        # TODO: æ¥å…¥ retrieval_expert
        # TODO: æ¥å…¥ reasoning_expert
        # TODO: æ¥å…¥ generation_expert
        # TODO: æ¥å…¥ expression_expert
        # TODO: æ¥å…¥ quality_expert
    ]
    
    # åˆ›å»º Deep Agent
    _deep_agent = create_deep_agent(
        model=model,
        tools=tools,
        system_prompt=EDUCATION_SYSTEM_PROMPT,
        subagents=subagents if subagents else None,
        checkpointer=_checkpointer,
        store=_store,
        debug=settings.DEBUG,
        name="education_agent",
    )
    
    logger.info("Deep Agent åˆ›å»ºå®Œæˆ")
    return _deep_agent


# ==================== è¿è¡Œå‡½æ•° ====================

async def run_deep_agent(
    query: str,
    user_id: str,
    book_id: str,
    book_name: str = "",
    book_subject: str = "",
    history: list = None,
    thread_id: str = None,
) -> Dict[str, Any]:
    """
    è¿è¡Œ Deep Agentï¼ˆéæµå¼ï¼‰

    Args:
        query: ç”¨æˆ·é—®é¢˜
        user_id: ç”¨æˆ·ID
        book_id: æ•™æID
        book_name: æ•™æåç§°
        book_subject: æ•™æå­¦ç§‘
        history: å¯¹è¯å†å²
        thread_id: å¯¹è¯çº¿ç¨‹ID

    Returns:
        {
            "answer": "æœ€ç»ˆå›ç­”",
            "error": None
        }
    """
    logger.info(f"è¿è¡Œ Deep Agent: query={query[:50]}..., thread_id={thread_id}")

    agent = get_deep_agent()

    # æ„å»ºæ¶ˆæ¯
    messages = []

    # æ·»åŠ å†å²æ¶ˆæ¯
    if history:
        for msg in history:
            messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", "")
            })

    # æ·»åŠ å½“å‰é—®é¢˜
    messages.append({"role": "user", "content": query})

    # æ„å»ºé…ç½®
    effective_thread_id = thread_id or f"{user_id}_{book_id}"
    config = {
        "configurable": {
            "thread_id": effective_thread_id,
            "user_id": user_id,
        }
    }

    # æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯ï¼ˆæ³¨å…¥ä¸Šä¸‹æ–‡ï¼‰
    # Deep Agent ä¼šè‡ªåŠ¨å¤„ç† system_promptï¼Œè¿™é‡Œé€šè¿‡æ¶ˆæ¯ä¼ é€’ä¸Šä¸‹æ–‡
    context_msg = f"[ä¸Šä¸‹æ–‡] ç”¨æˆ·ID: {user_id}, æ•™æID: {book_id}, æ•™æ: {book_name}, å­¦ç§‘: {book_subject}"
    messages.insert(0, {"role": "system", "content": context_msg})

    try:
        # è¿è¡Œ Agent
        result = await agent.ainvoke({"messages": messages}, config)

        # æå–æœ€ç»ˆå›ç­”
        final_message = result.get("messages", [])[-1] if result.get("messages") else None
        answer = final_message.content if final_message else ""

        return {
            "answer": answer,
            "error": None
        }

    except Exception as e:
        logger.error(f"Deep Agent è¿è¡Œå¤±è´¥: {e}")
        return {
            "answer": "",
            "error": str(e)
        }


async def run_deep_agent_stream(
    query: str,
    user_id: str,
    book_id: str,
    book_name: str = "",
    book_subject: str = "",
    history: list = None,
    thread_id: str = None,
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    è¿è¡Œ Deep Agentï¼ˆæµå¼ï¼‰

    ä½¿ç”¨å¤šæ¨¡å¼æµå¼è¾“å‡ºï¼š
    - updates: èŠ‚ç‚¹çŠ¶æ€æ›´æ–°
    - messages: LLM token æµå¼è¾“å‡º
    - custom: è‡ªå®šä¹‰è¿›åº¦ä¿¡æ¯ï¼ˆæ¥è‡ªå·¥å…·ï¼‰

    Yields:
        ä¸åŒç±»å‹çš„æµå¼äº‹ä»¶ï¼š
        - {"event_type": "node", "node": "agent", "status": "start/end"}
        - {"event_type": "token", "content": "..."}
        - {"event_type": "progress", "step": "memory_read", "message": "..."}
        - {"event_type": "error", "error": "..."}
    """
    logger.info(f"æµå¼è¿è¡Œ Deep Agent: query={query[:50]}..., thread_id={thread_id}")

    agent = get_deep_agent()

    # æ„å»ºæ¶ˆæ¯
    messages = []

    if history:
        for msg in history:
            messages.append({
                "role": msg.get("role", "user"),
                "content": msg.get("content", "")
            })

    messages.append({"role": "user", "content": query})

    # æ„å»ºé…ç½®
    effective_thread_id = thread_id or f"{user_id}_{book_id}"
    config = {
        "configurable": {
            "thread_id": effective_thread_id,
            "user_id": user_id,
        }
    }

    # æ³¨å…¥ä¸Šä¸‹æ–‡
    context_msg = f"[ä¸Šä¸‹æ–‡] ç”¨æˆ·ID: {user_id}, æ•™æID: {book_id}, æ•™æ: {book_name}, å­¦ç§‘: {book_subject}"
    messages.insert(0, {"role": "system", "content": context_msg})

    # å‘é€å¼€å§‹äº‹ä»¶
    yield {
        "event_type": "start",
        "message": "ğŸ¤” æ­£åœ¨åˆ†æé—®é¢˜...",
    }

    try:
        # ä½¿ç”¨å¤šæ¨¡å¼æµå¼è¾“å‡º
        async for stream_mode, chunk in agent.astream(
            {"messages": messages},
            config,
            stream_mode=["updates", "messages", "custom"]
        ):
            if stream_mode == "updates":
                # èŠ‚ç‚¹çŠ¶æ€æ›´æ–°
                for node_name, state in chunk.items():
                    if state is None:
                        continue

                    # å‘é€èŠ‚ç‚¹è¿›åº¦
                    yield {
                        "event_type": "node",
                        "node": node_name,
                        "status": "update",
                    }

                    # æå–æœ€ç»ˆå›ç­”ï¼ˆä» agent èŠ‚ç‚¹ï¼‰
                    if node_name == "agent" and isinstance(state, dict):
                        current_messages = state.get("messages", [])
                        if hasattr(current_messages, 'value'):
                            current_messages = current_messages.value
                        if isinstance(current_messages, list) and current_messages:
                            last_message = current_messages[-1]
                            if hasattr(last_message, 'content') and last_message.content:
                                yield {
                                    "event_type": "answer",
                                    "content": last_message.content,
                                }

            elif stream_mode == "messages":
                # LLM token æµå¼è¾“å‡º
                message_chunk, metadata = chunk
                if hasattr(message_chunk, 'content') and message_chunk.content:
                    yield {
                        "event_type": "token",
                        "content": message_chunk.content,
                        "node": metadata.get("langgraph_node", ""),
                    }

            elif stream_mode == "custom":
                # è‡ªå®šä¹‰è¿›åº¦ä¿¡æ¯ï¼ˆæ¥è‡ªå·¥å…·ï¼‰
                yield {
                    "event_type": "progress",
                    **chunk,  # åŒ…å« step, status, message, icon ç­‰
                }

    except Exception as e:
        logger.error(f"Deep Agent æµå¼è¿è¡Œå¤±è´¥: {e}")
        yield {
            "event_type": "error",
            "error": str(e),
        }

