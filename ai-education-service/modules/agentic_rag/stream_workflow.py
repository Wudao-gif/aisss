"""
Agentic RAG æµå¼å·¥ä½œæµ
æ”¯æŒå®æ—¶è¾“å‡º Agent æ€è€ƒè¿‡ç¨‹
"""

import logging
import json
from typing import List, Dict, Any, Optional, AsyncGenerator

from llama_index.core.workflow import (
    StartEvent,
    StopEvent,
    Workflow,
    step,
    Context,
)
import httpx

from config import settings
from .events import (
    QueryType,
    ProgressType,
    ProgressEvent,
    RouteDecisionEvent,
    PlanEvent,
    SubTask,
    ToolCallEvent,
    ToolResultEvent,
    RetryEvent,
    SynthesizeEvent,
)
from .tools import ToolRegistry, VectorSearchTool, CalculatorTool, KnowledgeGraphTool

logger = logging.getLogger(__name__)

MAX_RETRY = 3


class AgenticStreamWorkflow(Workflow):
    """
    æµå¼ Agentic RAG å·¥ä½œæµ

    ç‰¹ç‚¹:
    - æ¯ä¸ªæ­¥éª¤éƒ½å‘é€ ProgressEvent
    - æœ€ç»ˆç­”æ¡ˆæµå¼è¾“å‡º
    - å‰ç«¯å¯å®æ—¶æ˜¾ç¤º Agent æ€è€ƒè¿‡ç¨‹
    """

    def __init__(self, tool_registry: ToolRegistry = None, **kwargs):
        super().__init__(**kwargs)
        self.tool_registry = tool_registry or ToolRegistry()
        self.chat_model = settings.CHAT_MODEL

    async def _call_llm(self, messages: List[Dict], temperature: float = 0.3) -> str:
        """è°ƒç”¨ LLM"""
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{settings.OPENROUTER_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={"model": self.chat_model, "messages": messages, "temperature": temperature}
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]

    async def _stream_llm(self, messages: List[Dict]) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨ LLM"""
        async with httpx.AsyncClient(timeout=120.0) as client:
            async with client.stream(
                "POST",
                f"{settings.OPENROUTER_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={"model": self.chat_model, "messages": messages, "temperature": 0.7, "stream": True}
            ) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            content = chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue

    @step
    async def route(self, ctx: Context, ev: StartEvent) -> RouteDecisionEvent | StopEvent:
        """æ­¥éª¤1: è·¯ç”±å†³ç­–"""
        query = ev.query
        history = getattr(ev, 'history', None) or []

        # å‘é€è¿›åº¦äº‹ä»¶
        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.ROUTING,
            message="ğŸ¤” æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...",
            detail=f"é—®é¢˜: {query[:50]}..."
        ))

        route_prompt = f"""åˆ†æé—®é¢˜ç±»å‹ï¼ˆè¿”å›JSONï¼‰:
é—®é¢˜: {query}
{{"type": "simple|complex|clarify|chitchat", "reasoning": "ç†ç”±", "rewritten_query": "æ”¹å†™æŸ¥è¯¢"}}"""

        try:
            result = await self._call_llm([{"role": "user", "content": route_prompt}])
            if "```" in result:
                result = result.split("```")[1].replace("json", "").strip()
            parsed = json.loads(result)

            query_type = QueryType(parsed.get("type", "simple"))

            # å‘é€è·¯ç”±ç»“æœ
            type_names = {"simple": "ç®€å•é—®é¢˜", "complex": "å¤æ‚é—®é¢˜", "chitchat": "é—²èŠ", "clarify": "éœ€æ¾„æ¸…"}
            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.ROUTING,
                message=f"ğŸ“‹ é—®é¢˜ç±»å‹: {type_names.get(query_type.value, query_type.value)}",
                detail=parsed.get("reasoning", "")
            ))

            if query_type == QueryType.CHITCHAT:
                answer = await self._call_llm([
                    {"role": "system", "content": "ä½ æ˜¯å‹å¥½çš„æ•™è‚²åŠ©æ‰‹"},
                    {"role": "user", "content": query}
                ])
                return StopEvent(result={"answer": answer, "sources": [], "query_type": "chitchat"})

            if query_type == QueryType.CLARIFY:
                return StopEvent(result={
                    "answer": f"æ‚¨çš„é—®é¢˜ä¸å¤ªæ¸…æ¥šï¼Œèƒ½å¦è¯¦ç»†è¯´æ˜ï¼Ÿ{parsed.get('reasoning', '')}",
                    "sources": [], "query_type": "clarify"
                })

            return RouteDecisionEvent(
                query=query, query_type=query_type,
                reasoning=parsed.get("reasoning", ""),
                rewritten_query=parsed.get("rewritten_query", query),
                history=history,
                filter_expr=getattr(ev, 'filter_expr', None)
            )
        except Exception as e:
            logger.error(f"è·¯ç”±å¤±è´¥: {e}")
            return RouteDecisionEvent(
                query=query, query_type=QueryType.SIMPLE,
                reasoning=f"é»˜è®¤å¤„ç†", rewritten_query=query,
                history=history, filter_expr=getattr(ev, 'filter_expr', None)
            )

    @step
    async def plan(self, ctx: Context, ev: RouteDecisionEvent | RetryEvent) -> ToolCallEvent:
        """æ­¥éª¤2: ä»»åŠ¡è§„åˆ’"""
        if isinstance(ev, RetryEvent):
            query, retry_count = ev.query, ev.retry_count
            filter_expr, history = ev.filter_expr, ev.history
            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.RETRYING,
                message=f"ğŸ”„ ç¬¬ {retry_count} æ¬¡é‡è¯•: {ev.suggestions[:30]}..."
            ))
        else:
            query = ev.rewritten_query or ev.query
            retry_count, filter_expr, history = 0, ev.filter_expr, ev.history

            if ev.query_type == QueryType.SIMPLE:
                ctx.write_event_to_stream(ProgressEvent(
                    progress_type=ProgressType.SEARCHING,
                    message="ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™..."
                ))
                return ToolCallEvent(
                    query=query, tool_name="vector_search",
                    tool_args={"query": query, "top_k": 5, "filter_expr": filter_expr},
                    subtask_id="simple", history=history
                )

        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.PLANNING, message="ğŸ“ æ­£åœ¨è§„åˆ’æ£€ç´¢ç­–ç•¥..."
        ))

        try:
            plan_prompt = f"""åˆ†è§£é—®é¢˜: {query}
{self.tool_registry.get_tools_prompt()}
è¿”å›: {{"subtasks": [{{"id": "1", "query": "å­æŸ¥è¯¢", "tool": "vector_search"}}]}}"""
            result = await self._call_llm([{"role": "user", "content": plan_prompt}])
            if "```" in result:
                result = result.split("```")[1].replace("json", "").strip()
            parsed = json.loads(result)

            subtasks = [SubTask(id=str(i), description=t.get("query", query),
                       tool=t.get("tool", "vector_search"),
                       tool_args={"query": t.get("query", query), "top_k": 5})
                for i, t in enumerate(parsed.get("subtasks", [{"query": query}]))]

            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.PLANNING,
                message=f"ğŸ“‹ å·²è§„åˆ’ {len(subtasks)} ä¸ªä»»åŠ¡"
            ))

            plan = PlanEvent(query=query, subtasks=subtasks, reasoning="",
                           history=history, filter_expr=filter_expr, retry_count=retry_count)
            await ctx.set("plan", plan)
            await ctx.set("all_results", [])
            await ctx.set("current_task_idx", 0)

            first = subtasks[0]
            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.SEARCHING,
                message=f"ğŸ” æ‰§è¡Œä»»åŠ¡ 1/{len(subtasks)}: {first.description[:25]}..."
            ))
            return ToolCallEvent(query=query, tool_name=first.tool,
                tool_args={**first.tool_args, "filter_expr": filter_expr},
                subtask_id=first.id, plan=plan, history=history)
        except Exception as e:
            logger.error(f"è§„åˆ’å¤±è´¥: {e}")
            return ToolCallEvent(query=query, tool_name="vector_search",
                tool_args={"query": query, "top_k": 5, "filter_expr": filter_expr},
                subtask_id="fallback", history=history)

    @step
    async def execute_tool(self, ctx: Context, ev: ToolCallEvent) -> ToolResultEvent:
        """æ­¥éª¤3: æ‰§è¡Œå·¥å…·"""
        tool = self.tool_registry.get(ev.tool_name) or self.tool_registry.get("vector_search")
        if not tool:
            return ToolResultEvent(query=ev.query, tool_name=ev.tool_name, tool_args=ev.tool_args,
                result={"error": "æ— å·¥å…·"}, success=False, subtask_id=ev.subtask_id,
                plan=ev.plan, history=ev.history)

        try:
            result = await tool.execute(**ev.tool_args)
            all_results = await ctx.get("all_results", [])
            all_results.append({"tool": ev.tool_name, "result": result, "subtask_id": ev.subtask_id})
            await ctx.set("all_results", all_results)

            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.SEARCHING,
                message=f"âœ… æ‰¾åˆ° {result.get('count', 0)} æ¡å†…å®¹"
            ))
            return ToolResultEvent(query=ev.query, tool_name=ev.tool_name, tool_args=ev.tool_args,
                result=result, success=result.get("success", True), subtask_id=ev.subtask_id,
                plan=ev.plan, all_results=all_results, history=ev.history)
        except Exception as e:
            return ToolResultEvent(query=ev.query, tool_name=ev.tool_name, tool_args=ev.tool_args,
                result={}, success=False, error=str(e), subtask_id=ev.subtask_id,
                plan=ev.plan, history=ev.history)



    @step
    async def reflect(self, ctx: Context, ev: ToolResultEvent) -> SynthesizeEvent | ToolCallEvent | RetryEvent:
        """æ­¥éª¤4: åæ€"""
        plan = ev.plan or await ctx.get("plan")
        all_results = ev.all_results or await ctx.get("all_results", [])
        retry_count = plan.retry_count if plan else 0

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å­ä»»åŠ¡
        if plan and plan.subtasks:
            idx = await ctx.get("current_task_idx", 0)
            if idx + 1 < len(plan.subtasks):
                await ctx.set("current_task_idx", idx + 1)
                next_task = plan.subtasks[idx + 1]
                ctx.write_event_to_stream(ProgressEvent(
                    progress_type=ProgressType.SEARCHING,
                    message=f"ğŸ” æ‰§è¡Œä»»åŠ¡ {idx+2}/{len(plan.subtasks)}: {next_task.description[:25]}..."
                ))
                return ToolCallEvent(query=ev.query, tool_name=next_task.tool,
                    tool_args={**next_task.tool_args, "filter_expr": plan.filter_expr},
                    subtask_id=next_task.id, plan=plan, history=ev.history)

        # åæ€è¯„ä¼°
        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.REFLECTING, message="ğŸ§ æ­£åœ¨è¯„ä¼°æ£€ç´¢ç»“æœ..."
        ))

        context, sources = self._build_context(all_results)

        # LLM è¯„ä¼°
        if context:
            eval_result = await self._evaluate_with_llm(ev.query, context)
            if eval_result["decision"] == "retry" and retry_count < MAX_RETRY:
                ctx.write_event_to_stream(ProgressEvent(
                    progress_type=ProgressType.REFLECTING,
                    message=f"âš ï¸ ç»“æœä¸å¤Ÿå……åˆ†ï¼Œå‡†å¤‡é‡è¯•..."
                ))
                return RetryEvent(query=ev.query, reason=eval_result["reason"],
                    previous_results=all_results, suggestions=eval_result["suggestions"],
                    retry_count=retry_count + 1, history=ev.history,
                    filter_expr=plan.filter_expr if plan else None)

        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.REFLECTING,
            message=f"âœ… æ‰¾åˆ° {len(sources)} æ¡ç›¸å…³èµ„æ–™ï¼Œå‡†å¤‡ç”Ÿæˆç­”æ¡ˆ"
        ))
        return SynthesizeEvent(query=ev.query, results=all_results, context=context,
            sources=sources, history=ev.history)

    async def _evaluate_with_llm(self, query: str, context: str) -> Dict[str, Any]:
        """LLM è¯„ä¼°ç»“æœè´¨é‡"""
        prompt = f"""è¯„ä¼°æ£€ç´¢ç»“æœæ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼ˆè¿”å›JSONï¼‰:
é—®é¢˜: {query}
å†…å®¹: {context[:2000]}
{{"decision": "sufficient|retry", "reason": "ç†ç”±", "suggestions": "å»ºè®®"}}"""
        try:
            result = await self._call_llm([{"role": "user", "content": prompt}], 0.1)
            if "```" in result:
                result = result.split("```")[1].replace("json", "").strip()
            return json.loads(result)
        except:
            return {"decision": "sufficient", "reason": "", "suggestions": ""}

    def _build_context(self, all_results: List[Dict]) -> tuple:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        sources, parts = [], []
        for r in all_results:
            for item in r.get("result", {}).get("results", []):
                sources.append(item)
                parts.append(f"[æ¥æº{len(sources)}] {item.get('text', '')[:500]}")
        return "\n\n".join(parts), sources

    @step
    async def synthesize(self, ctx: Context, ev: SynthesizeEvent) -> StopEvent:
        """æ­¥éª¤5: æµå¼ç”Ÿæˆç­”æ¡ˆ"""
        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.SYNTHESIZING, message="âœ¨ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."
        ))

        if not ev.context:
            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.DONE, message="å®Œæˆ"
            ))
            return StopEvent(result={"answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", "sources": []})

        prompt = f"""åŸºäºèµ„æ–™å›ç­”é—®é¢˜ï¼Œæ ‡æ³¨æ¥æºå¦‚[æ¥æº1]ã€‚
èµ„æ–™:
{ev.context}
é—®é¢˜: {ev.query}"""

        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šæ•™è‚²åŠ©æ‰‹"}]
        if ev.history:
            messages.extend(ev.history[-4:])
        messages.append({"role": "user", "content": prompt})

        # æµå¼ç”Ÿæˆ
        answer_parts = []
        async for chunk in self._stream_llm(messages):
            answer_parts.append(chunk)
            ctx.write_event_to_stream(ProgressEvent(
                progress_type=ProgressType.STREAMING,
                message=chunk  # æ¯ä¸ª chunk ä½œä¸ºæ¶ˆæ¯å‘é€
            ))

        answer = "".join(answer_parts)

        ctx.write_event_to_stream(ProgressEvent(
            progress_type=ProgressType.DONE, message="âœ… å›ç­”å®Œæˆ"
        ))

        return StopEvent(result={"answer": answer, "sources": ev.sources, "has_context": True})


# ============ å·¥å‚å‡½æ•° ============

_stream_workflow: Optional[AgenticStreamWorkflow] = None


def get_agentic_stream_workflow() -> AgenticStreamWorkflow:
    """è·å–æµå¼ AgenticRAGWorkflow å•ä¾‹"""
    global _stream_workflow
    if _stream_workflow is None:
        from ..vector_store import VectorStore
        from ..document_processor import get_embedding_model

        registry = ToolRegistry()
        vector_store = VectorStore()
        embedding_model = get_embedding_model()

        registry.register(VectorSearchTool(vector_store, embedding_model))
        registry.register(CalculatorTool())
        registry.register(KnowledgeGraphTool())

        _stream_workflow = AgenticStreamWorkflow(
            tool_registry=registry, timeout=180, verbose=False
        )
        logger.info(f"AgenticStreamWorkflow åˆå§‹åŒ–å®Œæˆï¼Œ{len(registry._tools)} ä¸ªå·¥å…·")
    return _stream_workflow
