"""
API è·¯ç”±å®šä¹‰

ç»Ÿä¸€ä½¿ç”¨ LangGraph å¤šæ™ºèƒ½ä½“æ¶æ„ (v4)
"""

import json
import logging
from pathlib import Path
from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from fastapi.responses import StreamingResponse

from .schemas import (
    ProcessDocumentRequest,
    ProcessDocumentResponse,
    HealthResponse,
    ErrorResponse,
    SearchRequest,
    SearchResponse,
    SearchResult,
    ChatRequest,
    ChatResponse,
    ChatResumeRequest,
)
from .dependencies import verify_api_key
from modules import ProcessingPipeline, RAGRetriever
from modules.document_workflow import get_document_workflow
from modules.langgraph import run_deep_agent, run_deep_agent_stream
from config import settings

logger = logging.getLogger(__name__)

router = APIRouter()

# å…¨å±€å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_pipeline: ProcessingPipeline = None
_retriever: RAGRetriever = None


def get_pipeline() -> ProcessingPipeline:
    """è·å–å¤„ç†ç®¡é“å®ä¾‹"""
    global _pipeline
    if _pipeline is None:
        _pipeline = ProcessingPipeline()
    return _pipeline


def get_retriever() -> RAGRetriever:
    """è·å– RAG æ£€ç´¢å™¨å®ä¾‹"""
    global _retriever
    if _retriever is None:
        _retriever = RAGRetriever()
    return _retriever


@router.get(
    "/health",
    response_model=HealthResponse,
    summary="å¥åº·æ£€æŸ¥",
    description="æ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"
)
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return HealthResponse(
        status="healthy",
        version=settings.APP_VERSION
    )


@router.post(
    "/process-document",
    response_model=ProcessDocumentResponse,
    responses={
        200: {"model": ProcessDocumentResponse, "description": "å¤„ç†æˆåŠŸ"},
        400: {"model": ErrorResponse, "description": "è¯·æ±‚å‚æ•°é”™è¯¯"},
        401: {"model": ErrorResponse, "description": "è®¤è¯å¤±è´¥"},
        500: {"model": ErrorResponse, "description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    },
    summary="å¤„ç†æ–‡æ¡£",
    description="ä» OSS ä¸‹è½½æ–‡æ¡£ï¼Œè¿›è¡Œè§£æã€åˆ†å—ã€å‘é‡åŒ–ï¼Œå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼Œå¹¶æå–çŸ¥è¯†å›¾è°±"
)
async def process_document(
    request: ProcessDocumentRequest,
    _: bool = Depends(verify_api_key)
):
    """
    å¤„ç†æ–‡æ¡£ç«¯ç‚¹ï¼ˆä½¿ç”¨ Workflowï¼‰

    æ¥æ”¶ OSS æ–‡ä»¶ä¿¡æ¯ï¼Œæ‰§è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹ï¼š
    1. ä» OSS ä¸‹è½½æ–‡ä»¶
    2. ä½¿ç”¨ LlamaIndex è§£ææ–‡æ¡£
    3. æ–‡æœ¬åˆ†å—
    4. ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨åˆ° DashVector
    5. æå–çŸ¥è¯†å›¾è°±å¹¶å­˜å‚¨åˆ° Neo4j
    """
    try:
        logger.info(f"[Workflow] æ”¶åˆ°å¤„ç†è¯·æ±‚: {request.oss_key}")

        workflow = get_document_workflow()
        result = await workflow.run(
            oss_key=request.oss_key,
            bucket=request.bucket,
            metadata=request.metadata or {}
        )

        if result.success:
            return ProcessDocumentResponse(
                success=True,
                message=result.message,
                data=result.to_dict()
            )
        else:
            return ProcessDocumentResponse(
                success=False,
                message=result.message,
                data=result.to_dict()
            )

    except Exception as e:
        logger.error(f"[Workflow] å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


@router.post(
    "/process-document/async",
    response_model=ProcessDocumentResponse,
    summary="å¼‚æ­¥å¤„ç†æ–‡æ¡£",
    description="å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼Œç«‹å³è¿”å›ï¼Œåå°æ‰§è¡Œå¤„ç†ï¼ˆä½¿ç”¨ Workflowï¼ŒåŒ…å«çŸ¥è¯†å›¾è°±æå–ï¼‰"
)
async def process_document_async(
    request: ProcessDocumentRequest,
    background_tasks: BackgroundTasks,
    _: bool = Depends(verify_api_key)
):
    """
    å¼‚æ­¥å¤„ç†æ–‡æ¡£ç«¯ç‚¹ï¼ˆä½¿ç”¨ Workflowï¼‰

    ç«‹å³è¿”å›å“åº”ï¼Œåœ¨åå°æ‰§è¡Œå¤„ç†ä»»åŠ¡ã€‚
    åŒ…å«çŸ¥è¯†å›¾è°±æå–åŠŸèƒ½ã€‚
    """
    import asyncio

    async def background_process_async():
        try:
            logger.info(f"[Workflow] åå°å¼€å§‹å¤„ç†: {request.oss_key}")
            workflow = get_document_workflow()
            result = await workflow.run(
                oss_key=request.oss_key,
                bucket=request.bucket,
                metadata=request.metadata or {}
            )
            logger.info(f"[Workflow] åå°å¤„ç†å®Œæˆ: {request.oss_key}, ç»“æœ: {result.success}, KG: {result.kg_entities} å®ä½“, {result.kg_relations} å…³ç³»")
        except Exception as e:
            logger.error(f"[Workflow] åå°å¤„ç†å¤±è´¥: {request.oss_key}, é”™è¯¯: {e}")

    def background_process():
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(background_process_async())
        finally:
            loop.close()

    background_tasks.add_task(background_process)

    return ProcessDocumentResponse(
        success=True,
        message="ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°å¤„ç†ï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±æå–ï¼‰",
        data={
            "status": "pending",
            "file_key": request.oss_key
        }
    )


# ==================== RAG æ£€ç´¢æ¥å£ ====================

@router.post(
    "/search",
    response_model=SearchResponse,
    summary="å‘é‡æ£€ç´¢",
    description="æ ¹æ®æŸ¥è¯¢æ–‡æœ¬æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ"
)
async def search(
    request: SearchRequest,
    _: bool = Depends(verify_api_key)
):
    """
    å‘é‡æ£€ç´¢ç«¯ç‚¹

    æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆå‘é‡ï¼Œåœ¨å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ç‰‡æ®µã€‚
    """
    try:
        logger.info(f"æ”¶åˆ°æ£€ç´¢è¯·æ±‚: {request.query[:50]}...")

        retriever = get_retriever()
        results = retriever.retrieve(
            query=request.query,
            top_k=request.top_k,
            filter_expr=request.filter_expr
        )

        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        search_results = [
            SearchResult(
                id=r["id"],
                text=r["text"],
                score=r["score"],
                metadata=r.get("metadata")
            )
            for r in results
        ]

        return SearchResponse(
            success=True,
            results=search_results,
            total=len(search_results)
        )

    except Exception as e:
        logger.error(f"æ£€ç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


# ==================== å‘é‡ç®¡ç†æ¥å£ ====================

@router.delete(
    "/vectors/{book_id}",
    summary="åˆ é™¤å›¾ä¹¦å‘é‡",
    description="åˆ é™¤æŒ‡å®šå›¾ä¹¦çš„æ‰€æœ‰å‘é‡æ•°æ®"
)
async def delete_vectors(
    book_id: str,
    _: bool = Depends(verify_api_key)
):
    """
    åˆ é™¤å›¾ä¹¦å‘é‡ç«¯ç‚¹

    æ ¹æ® book_id åˆ é™¤è¯¥å›¾ä¹¦çš„æ‰€æœ‰å‘é‡æ•°æ®ã€‚
    ç”¨äºå›¾ä¹¦åˆ é™¤æˆ–æ›´æ–°æ—¶æ¸…ç†æ—§æ•°æ®ã€‚
    """
    try:
        logger.info(f"æ”¶åˆ°åˆ é™¤å‘é‡è¯·æ±‚: book_id={book_id}")

        retriever = get_retriever()
        success = retriever.vector_store.delete_by_filter(f"book_id = '{book_id}'")

        return {
            "success": success,
            "message": "å‘é‡åˆ é™¤æˆåŠŸ" if success else "å‘é‡åˆ é™¤å¤±è´¥",
            "book_id": book_id
        }

    except Exception as e:
        logger.error(f"åˆ é™¤å‘é‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


# ==================== æ™ºèƒ½é—®ç­”æ¥å£ (LangGraph å¤šæ™ºèƒ½ä½“) ====================

@router.post(
    "/chat",
    response_model=ChatResponse,
    summary="æ™ºèƒ½é—®ç­”",
    description="åŸºäº Deep Agent ä¸»ç³»ç»Ÿçš„æ™ºèƒ½é—®ç­”ï¼Œæ”¯æŒè®°å¿†ã€ä»»åŠ¡è§„åˆ’ã€å­ä»£ç†å§”æ‰˜"
)
async def chat(
    request: ChatRequest,
    _: bool = Depends(verify_api_key)
):
    """
    æ™ºèƒ½é—®ç­”æ¥å£ï¼ˆDeep Agent ä¸»ç³»ç»Ÿï¼‰

    ç‰¹æ€§ï¼š
    - Deep Agent ä½œä¸ºä¸»ç³»ç»Ÿ
    - è‡ªåŠ¨ä»»åŠ¡è§„åˆ’ï¼ˆTodoListMiddlewareï¼‰
    - æ–‡ä»¶ç³»ç»Ÿè®¿é—®ï¼ˆFilesystemMiddlewareï¼‰
    - å­ä»£ç†å§”æ‰˜ï¼ˆSubAgentMiddlewareï¼‰
    - é•¿æœŸè®°å¿†ï¼ˆmemory_read/memory_writeï¼‰
    """
    try:
        logger.info(f"[Deep Agent] æ”¶åˆ°é—®ç­”è¯·æ±‚: {request.question[:50]}...")

        # è½¬æ¢å†å²å¯¹è¯æ ¼å¼
        history = None
        if request.history:
            history = [{"role": msg.role, "content": msg.content} for msg in request.history]

        # æ„å»º thread_idï¼ˆç”¨äºçŸ­æœŸè®°å¿†æŒä¹…åŒ–ï¼‰
        user_id = request.user_id or "anonymous"
        book_id = request.book_id or "default"
        thread_id = request.thread_id or f"{user_id}_{book_id}"

        # è¿è¡Œ Deep Agent
        result = await run_deep_agent(
            query=request.question,
            user_id=user_id,
            book_id=book_id,
            book_name=request.book_name or "",
            book_subject="",
            history=history,
            thread_id=thread_id
        )

        # Deep Agent è¿”å›æ ¼å¼ï¼š{"answer": "...", "error": None}
        if result.get("error"):
            raise Exception(result["error"])

        return ChatResponse(
            success=True,
            answer=result.get("answer", ""),
            sources=[],  # Deep Agent æš‚ä¸è¿”å›æ¥æº
            has_context=False
        )

    except Exception as e:
        logger.error(f"[Deep Agent] é—®ç­”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


@router.post(
    "/chat/stream",
    summary="æ™ºèƒ½æµå¼é—®ç­”",
    description="åŸºäº Deep Agent ä¸»ç³»ç»Ÿçš„æµå¼é—®ç­”ï¼Œè¾“å‡ºå¤„ç†è¿›åº¦å’Œç­”æ¡ˆ"
)
async def chat_stream(
    request: ChatRequest,
    _: bool = Depends(verify_api_key)
):
    """
    æ™ºèƒ½æµå¼é—®ç­”æ¥å£ï¼ˆDeep Agent ä¸»ç³»ç»Ÿï¼‰

    SSE äº‹ä»¶æ ¼å¼:
    - start: å¼€å§‹å¤„ç†
    - progress: å¤„ç†è¿›åº¦ï¼ˆæ¥è‡ªå·¥å…·çš„è‡ªå®šä¹‰è¿›åº¦ï¼‰
    - node: èŠ‚ç‚¹çŠ¶æ€æ›´æ–°
    - token: LLM token æµå¼è¾“å‡ºï¼ˆé€å­—ï¼‰
    - answer: å®Œæ•´å›ç­”
    - error: é”™è¯¯ä¿¡æ¯
    - done: å®Œæˆæ ‡è®°
    """
    async def generate_stream():
        try:
            logger.info(f"[Deep Agent Stream] é—®é¢˜: {request.question[:50]}...")

            # è½¬æ¢å†å²å¯¹è¯æ ¼å¼
            history = None
            if request.history:
                history = [{"role": msg.role, "content": msg.content} for msg in request.history]

            # æ„å»º thread_idï¼ˆç”¨äºçŸ­æœŸè®°å¿†æŒä¹…åŒ–ï¼‰
            user_id = request.user_id or "anonymous"
            book_id = request.book_id or "default"
            thread_id = request.thread_id or f"{user_id}_{book_id}"

            # æµå¼è¿è¡Œ Deep Agent
            async for event in run_deep_agent_stream(
                query=request.question,
                user_id=user_id,
                book_id=book_id,
                book_name=request.book_name or "",
                book_subject="",
                history=history,
                thread_id=thread_id
            ):
                event_type = event.get("event_type", "")

                if event_type == "start":
                    # å¼€å§‹äº‹ä»¶
                    yield f"data: {json.dumps({'type': 'start', 'message': event.get('message', 'å¼€å§‹å¤„ç†...')}, ensure_ascii=False)}\n\n"

                elif event_type == "node":
                    # èŠ‚ç‚¹çŠ¶æ€æ›´æ–° - å¿½ç•¥ï¼Œä¸å‘é€è™šå‡çš„è¿›åº¦æ¶ˆæ¯
                    # åªæœ‰å½“å·¥å…·å‘é€è‡ªå®šä¹‰è¿›åº¦æ—¶ï¼Œæ‰æ˜¾ç¤ºæ­¥éª¤
                    pass

                elif event_type == "progress":
                    # è‡ªå®šä¹‰è¿›åº¦ï¼ˆæ¥è‡ªå·¥å…·çš„çœŸå®è¿›åº¦ï¼‰
                    yield f"data: {json.dumps({'type': 'progress', 'step': event.get('step', ''), 'status': event.get('status', ''), 'message': event.get('message', ''), 'icon': event.get('icon', '')}, ensure_ascii=False)}\n\n"

                elif event_type == "token":
                    # LLM token æµå¼è¾“å‡ºï¼ˆé€å­—ï¼‰
                    content = event.get("content", "")
                    if content:
                        yield f"data: {json.dumps({'type': 'token', 'data': content}, ensure_ascii=False)}\n\n"

                elif event_type == "answer":
                    # å®Œæ•´å›ç­”ï¼ˆå¤‡ç”¨ï¼Œç”¨äºéæµå¼åœºæ™¯ï¼‰
                    content = event.get("content", "")
                    if content:
                        yield f"data: {json.dumps({'type': 'answer', 'data': content}, ensure_ascii=False)}\n\n"

                elif event_type == "interrupt":
                    # HITL ä¸­æ–­ - éœ€è¦ç”¨æˆ·å®¡æ‰¹
                    logger.info("ğŸ›‘ [API] æ£€æµ‹åˆ° HITL ä¸­æ–­ï¼Œè½¬å‘ç»™å‰ç«¯")
                    interrupt_data = event.get("interrupt", {})
                    yield f"data: {json.dumps({'type': '__interrupt__', 'data': interrupt_data}, ensure_ascii=False)}\n\n"
                    # ä¸å‘é€ doneï¼Œç­‰å¾…å‰ç«¯æ¢å¤

                elif event_type == "error":
                    # é”™è¯¯
                    yield f"data: {json.dumps({'type': 'error', 'message': event.get('error', 'æœªçŸ¥é”™è¯¯')}, ensure_ascii=False)}\n\n"
                    break

            # å®Œæˆ
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            logger.error(f"[Deep Agent Stream] é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.post(
    "/chat/resume",
    summary="æ¢å¤ HITL ä¸­æ–­æ‰§è¡Œ",
    description="å¤„ç†ç”¨æˆ·å¯¹ Human-in-the-Loop ä¸­æ–­çš„å†³ç­–ï¼Œæ¢å¤ Deep Agent æ‰§è¡Œ"
)
async def chat_resume(
    request: ChatResumeRequest,
    _: bool = Depends(verify_api_key)
):
    """
    æ¢å¤ HITL ä¸­æ–­æ‰§è¡Œæ¥å£

    å½“ Deep Agent åœ¨æ‰§è¡Œæ•æ„Ÿæ“ä½œï¼ˆå¦‚ memory_writeï¼‰æ—¶ï¼Œä¼šä¸­æ–­å¹¶ç­‰å¾…ç”¨æˆ·å®¡æ‰¹ã€‚
    ç”¨æˆ·åšå‡ºå†³ç­–åï¼Œé€šè¿‡æ­¤æ¥å£æ¢å¤æ‰§è¡Œã€‚

    SSE äº‹ä»¶æ ¼å¼ä¸ /chat/stream ç›¸åŒï¼š
    - progress: å¤„ç†è¿›åº¦
    - token: LLM token æµå¼è¾“å‡º
    - answer: å®Œæ•´å›ç­”
    - error: é”™è¯¯ä¿¡æ¯
    - done: å®Œæˆæ ‡è®°
    """
    async def generate_stream():
        try:
            from langgraph.types import Command
            from modules.langgraph import get_deep_agent
            from modules.langgraph.hitl_handler import validate_decisions

            logger.info(f"[HITL Resume] æ¢å¤æ‰§è¡Œ: thread_id={request.thread_id}, decisions={len(request.decisions)}")

            agent = get_deep_agent()

            # æ„å»ºé…ç½®ï¼ˆä½¿ç”¨ç›¸åŒçš„ thread_id ä»¥æ¢å¤çŠ¶æ€ï¼‰
            config = {
                "configurable": {
                    "thread_id": request.thread_id,
                }
            }

            # è½¬æ¢å†³ç­–æ ¼å¼
            # å¯¹äº memory_write çš„ HITLï¼Œå†³ç­–åº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« action å’Œå…¶ä»–ä¿¡æ¯
            decisions = []
            for d in request.decisions:
                decision = {
                    "type": d.type,
                }
                if d.edited_action:
                    decision["edited_action"] = d.edited_action
                decisions.append(decision)

            # åˆ›å»ºæ¢å¤å‘½ä»¤
            # å¯¹äº memory_write çš„ä¸­æ–­ï¼Œæ¢å¤å€¼åº”è¯¥æ˜¯ {"action": "approve"} æˆ– {"action": "reject"}
            if len(decisions) == 1 and decisions[0].get("type") == "approve":
                resume_value = {"action": "approve"}
            elif len(decisions) == 1 and decisions[0].get("type") == "reject":
                resume_value = {"action": "reject"}
            elif len(decisions) == 1 and decisions[0].get("type") == "edit":
                resume_value = {
                    "action": "edit",
                    "edited_action": decisions[0].get("edited_action")
                }
            else:
                # å¤šä¸ªå†³ç­–æˆ–å…¶ä»–æƒ…å†µ
                resume_value = {"decisions": decisions}

            resume_command = Command(resume=resume_value)

            logger.info(f"[HITL Resume] åˆ›å»ºæ¢å¤å‘½ä»¤: {len(decisions)} ä¸ªå†³ç­–")

            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'message': 'ğŸ”„ æ¢å¤æ‰§è¡Œ...'}, ensure_ascii=False)}\n\n"

            # ä½¿ç”¨ updates æ¨¡å¼æµå¼è¾“å‡ºæ¢å¤æ‰§è¡Œ
            async for chunk in agent.astream(
                resume_command,
                config,
                stream_mode="updates"
            ):
                # å¤„ç† updates æ¨¡å¼çš„è¾“å‡º
                if isinstance(chunk, dict):
                    for node_name, state in chunk.items():
                        if state is None:
                            continue

                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ä¸­æ–­ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
                        if isinstance(state, dict) and "__interrupt__" in state:
                            logger.warning(f"ğŸ›‘ [HITL Resume] æ¢å¤è¿‡ç¨‹ä¸­åˆå‡ºç°ä¸­æ–­")
                            interrupt_data = state.get("__interrupt__", [])
                            if interrupt_data:
                                yield f"data: {json.dumps({'type': '__interrupt__', 'data': interrupt_data[0].value if hasattr(interrupt_data[0], 'value') else interrupt_data[0]}, ensure_ascii=False)}\n\n"
                            return

                        # æå–æœ€ç»ˆå›ç­”ï¼ˆä» agent èŠ‚ç‚¹ï¼‰
                        if node_name == "agent" and isinstance(state, dict):
                            current_messages = state.get("messages", [])
                            if hasattr(current_messages, 'value'):
                                current_messages = current_messages.value
                            if isinstance(current_messages, list) and current_messages:
                                last_message = current_messages[-1]
                                if hasattr(last_message, 'content') and last_message.content:
                                    yield f"data: {json.dumps({'type': 'answer', 'data': last_message.content}, ensure_ascii=False)}\n\n"

            # å®Œæˆ
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            logger.error(f"[HITL Resume] æ¢å¤æ‰§è¡Œå¤±è´¥: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )